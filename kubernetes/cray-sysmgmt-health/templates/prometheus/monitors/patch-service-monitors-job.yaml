apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ include "cray-sysmgmt-health.fullname" . }}-patch-service-monitors-{{ .Release.Revision }}"
  labels:
    app.kubernetes.io/name: {{ include "cray-sysmgmt-health.fullname" . }}-patch-service-monitors
spec:
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      serviceAccountName: "patch-service-monitors"
      containers:
        - name: "patch-service-monitors"
          image: "{{ .Values.kubectl.repository }}:{{ .Values.kubectl.tag }}"
          command:
            - '/bin/sh'
          args:
            - '-c'
            - 'until kubectl get servicemonitor -n sysmgmt-health cray-sysmgmt-health-promet-kubelet > /dev/null 2>&1; do echo "Waiting for cray-sysmgmt-health-promet-kubelet service monitor to be created"; sleep 5; done; /usr/local/sbin/fix-kube-proxy-target-down-alert.sh; /usr/local/sbin/fix-kubelet-target-down-alert.sh; fix-kube-controller-manager-target-down-alert.sh; fix-kube-scheduler-target-down-alert.sh'
          volumeMounts:
          - mountPath: /usr/local/sbin
            name: patch-service-monitors
      volumes:
      - name: patch-service-monitors
        configMap:
          name: patch-service-monitors
          defaultMode: 0755
